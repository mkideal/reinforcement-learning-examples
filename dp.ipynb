{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements Dynamic Programming Algorithm for MDP probrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DP:\n",
    "    def compute_value(self, env, policy, values, state):\n",
    "        value = 0\n",
    "        gamma = env.gamma()\n",
    "        for action, pi in policy[state].items():\n",
    "            for (next_state, reward, prob) in env.step_enum(state=state, action=action):\n",
    "                value += pi * prob * (reward + gamma * values[next_state])\n",
    "        return value\n",
    "\n",
    "    def compute_value_from_qvalues(self, env, policy, qvalues, state):\n",
    "        value = 0\n",
    "        for action, pi in policy[state].items():\n",
    "            value += pi * qvalues[state][action]\n",
    "        return value\n",
    "\n",
    "    def compute_qvalue(self, env, policy, qvalues, state, action):\n",
    "        value = 0\n",
    "        gamma = env.gamma()\n",
    "        for (next_state, reward, prob) in env.step_enum(state=state, action=action):\n",
    "            value += prob * reward\n",
    "            for next_action, pi in policy[next_state].items():\n",
    "                value += prob * gamma * pi * qvalues[next_state][next_action]\n",
    "        return value\n",
    "\n",
    "    def compute_qvalue_from_values(self, env, policy, values, state, action):\n",
    "        value = 0\n",
    "        for (next_state, reward, prob) in env.step_enum(state=state, action=action):\n",
    "            value += prob * (reward + values[next_state])\n",
    "        return value\n",
    "\n",
    "    def make_decision(self, policy, state):\n",
    "        sum = 0\n",
    "        actions = []\n",
    "        for action, pi in policy[state].items():\n",
    "            sum += pi\n",
    "            actions.append((action, sum))\n",
    "        sample = random.random() * sum\n",
    "        for (action, acc) in actions:\n",
    "            if sample < acc:\n",
    "                return action\n",
    "        return None\n",
    "\n",
    "    def evaluate(self, env, policy, values):\n",
    "        max_delta = 0\n",
    "        for state in range(len(values)):\n",
    "            new_value = self.compute_value(env, policy, values, state)\n",
    "            delta = abs(new_value - values[state])\n",
    "            max_delta = max(delta, max_delta)\n",
    "            values[state] = new_value\n",
    "        return max_delta\n",
    "\n",
    "    def qevaluate(self, env, policy, qvalues):\n",
    "        max_delta = 0\n",
    "        for state in range(len(qvalues)):\n",
    "            for action in qvalues[state]:\n",
    "                new_value = self.compute_qvalue(env, policy, qvalues, state, action)\n",
    "                delta = abs(new_value - qvalues[state][action])\n",
    "                max_delta = max(delta, max_delta)\n",
    "                qvalues[state][action] = new_value\n",
    "        return max_delta\n",
    "\n",
    "    def policy_improvment(self, env, policy, values):\n",
    "        stable = True\n",
    "        for state in range(len(values)):\n",
    "            action = self.make_decision(policy, state)\n",
    "            if action is None:\n",
    "                continue\n",
    "            max_value = None\n",
    "            max_action = action\n",
    "            for action, prob in policy[state].items():\n",
    "                value = self.compute_qvalue_from_values(env, policy, values, state, action)\n",
    "                if max_value is None or value > max_value:\n",
    "                    max_value = value\n",
    "                    max_action = action\n",
    "            policy[state] = {}\n",
    "            policy[state][max_action] = 1\n",
    "            if action != max_action:\n",
    "                stable = False\n",
    "        return stable\n",
    "\n",
    "    def policy_iteration(self, env):\n",
    "        policy = env.default_policy()\n",
    "        values = [0] * env.num_states()\n",
    "        theta = 0.001\n",
    "        stable = False\n",
    "        times = 0\n",
    "        while not stable:\n",
    "            # policy evaluation\n",
    "            delta = theta+1\n",
    "            while delta > theta:\n",
    "                delta = self.evaluate(env, policy, values)\n",
    "            # policy improvment\n",
    "            stable = self.policy_improvment(env, policy, values)\n",
    "            times += 1\n",
    "            print(\"policy_iteration: times=%d\" % times)\n",
    "        return (policy, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: 4x4 gridworld shown below:\n",
    "\n",
    "|  | 1 | 2 | 3 |\n",
    "|---|---|---|---|\n",
    "| 4 | 5 | 6 | 7 |\n",
    "| 8 | 9 | 10| 11|\n",
    "| 12| 13| 14|   |\n",
    "\n",
    "1. Rt = 1 on all transitions.\n",
    "2. The nonterminal states S = {1,2,...,14}.\n",
    "3. There are four actions A = {up, down, left, right}, which deterministically cause the corresponding state transitions, except that actions would take the agent off the grid in fact leave the state unchanged.\n",
    "4. This is an undiscounted, episodic task, and the terminal states are the empty grids.\n",
    "\n",
    "Values for equiprobable random policy:\n",
    "\n",
    "|0|-14|-20|-22|\n",
    "|---|---|---|---|\n",
    "|-14|-18|-20|-20|\n",
    "|-20|-20|-18|-14|\n",
    "|-22|-20|-14|0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class GridWorld4x4:\n",
    "    ROWS = 4\n",
    "    COLUMNS = ROWS\n",
    "\n",
    "    class Action(Enum):\n",
    "        UP = 1\n",
    "        DOWN = 2\n",
    "        LEFT = 3\n",
    "        RIGHT = 4\n",
    "\n",
    "    def gamma(self):\n",
    "        return 1\n",
    "\n",
    "    def num_states(self):\n",
    "        return self.ROWS * self.COLUMNS\n",
    "\n",
    "    def step_enum(self, state, action):\n",
    "        column = int(state % self.COLUMNS)\n",
    "        row = int((state - column) / self.COLUMNS)\n",
    "        reward = -1\n",
    "        if action == self.Action.LEFT:\n",
    "            return [(state if column == 0 else self.make_state(row, column-1), reward, 1)]\n",
    "        elif action == self.Action.RIGHT:\n",
    "            return [(state if column+1 == self.COLUMNS else self.make_state(row, column+1), reward, 1)]\n",
    "        elif action == self.Action.UP:\n",
    "            return [(state if row == 0 else self.make_state(row-1, column), reward, 1)]\n",
    "        else:\n",
    "            return [(state if row+1 == self.ROWS else self.make_state(row+1, column), reward, 1)]\n",
    "    \n",
    "    def make_state(self, row, column):\n",
    "        return row * self.COLUMNS + column\n",
    "\n",
    "    def default_policy(self):\n",
    "        policy = {}\n",
    "        prob = 1.0 / 4.0\n",
    "        for state in range(self.num_states()):\n",
    "            if state == 0 or state == 15:\n",
    "                policy[state] = {}\n",
    "            else:\n",
    "                policy[state] = {\n",
    "                    self.Action.UP: prob,\n",
    "                    self.Action.DOWN: prob,\n",
    "                    self.Action.LEFT: prob,\n",
    "                    self.Action.RIGHT: prob\n",
    "                }\n",
    "        return policy\n",
    "\n",
    "    def print_values(self, title, values, formatter = None):\n",
    "        print(title)\n",
    "        for i in range(self.ROWS):\n",
    "            print('|', end='')\n",
    "            for j in range(self.COLUMNS):\n",
    "                state = self.make_state(i, j)\n",
    "                if formatter is not None:\n",
    "                    print(\"%s|\" % formatter(values[state]), end='')\n",
    "                else:\n",
    "                    print(\"%g|\" % round(values[state], 2), end='')\n",
    "            print()\n",
    "            if (i == 0):\n",
    "                print('|', end='')\n",
    "                for j in range(self.COLUMNS):\n",
    "                    print('---|', end='')\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_iteration: times=1\n",
      "policy_iteration: times=2\n",
      "policy improvment policy:\n",
      "||←|←|←|\n",
      "|---|---|---|---|\n",
      "|↑|←|←|↓|\n",
      "|↑|↑|↓|↓|\n",
      "|↑|→|→||\n",
      "policy improvment values:\n",
      "|0|-1|-2|-3|\n",
      "|---|---|---|---|\n",
      "|-1|-2|-3|-2|\n",
      "|-2|-3|-2|-1|\n",
      "|-3|-2|-1|0|\n"
     ]
    }
   ],
   "source": [
    "def gridworld_evaluate():\n",
    "    env = GridWorld4x4()\n",
    "    agent = DP()\n",
    "    values = [0] * (env.num_states())\n",
    "    max_times = 100000\n",
    "    times = 0\n",
    "    theta = 0.0001\n",
    "    delta = 1\n",
    "    policy = env.default_policy()\n",
    "    while delta > theta and times < max_times:\n",
    "        times += 1\n",
    "        delta = agent.evaluate(env, policy, values)\n",
    "    env.print_values(\"evaluate values:\", values)\n",
    "\n",
    "    delta = 1\n",
    "    times = 0\n",
    "    qvalues = []\n",
    "    for state in range(env.ROWS * env.COLUMNS):\n",
    "        qvalues.append({action: 0 for action in policy[state]})\n",
    "    while delta > theta and times < max_times:\n",
    "        times += 1\n",
    "        delta = agent.qevaluate(env, policy, qvalues)\n",
    "    print( \"evaluate qvalues: delta=%.6f, times=%d, q(11,down)=%g, q(7,down)=%g\" % (\n",
    "        delta,\n",
    "        times,\n",
    "        round(qvalues[11][env.Action.DOWN], 2),\n",
    "        round(qvalues[7][env.Action.DOWN], 2)\n",
    "    ))\n",
    "\n",
    "def format_policy(policy):\n",
    "    for action in policy:\n",
    "        if action == GridWorld4x4.Action.LEFT:\n",
    "            return \"←\"\n",
    "        elif action == GridWorld4x4.Action.RIGHT:\n",
    "            return \"→\"\n",
    "        elif action == GridWorld4x4.Action.UP:\n",
    "            return \"↑\"\n",
    "        elif action == GridWorld4x4.Action.DOWN:\n",
    "            return \"↓\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def gridworld_policy_improvment():\n",
    "    env = GridWorld4x4()\n",
    "    agent = DP()\n",
    "    (policy, values) = agent.policy_iteration(env)\n",
    "    env.print_values(\"policy improvment policy:\", policy, format_policy)\n",
    "    env.print_values(\"policy improvment values:\", values)\n",
    "\n",
    "gridworld_policy_improvment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy improvment policy:\n",
    "\n",
    "||←|←|←|\n",
    "|---|---|---|---|\n",
    "|↑|←|←|↓|\n",
    "|↑|↑|↓|↓|\n",
    "|↑|→|→||\n",
    "\n",
    "policy improvment values:\n",
    "\n",
    "|0|-1|-2|-3|\n",
    "|---|---|---|---|\n",
    "|-1|-2|-3|-2|\n",
    "|-2|-3|-2|-1|\n",
    "|-3|-2|-1|0|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
